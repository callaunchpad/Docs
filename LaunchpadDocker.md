### Docker Description
Docker is a lightweight virtualization system (similar to VMWare of Virtual Box). What makes docker special is its lightweight nature. It achieves this more directly integrating with the host operating system (a side effect of this is that it doesn’t tend to be as secure). This lightweight nature means that you can run many containers on a single machine with relative isolation, which is a very appealing feature for a service oriented architecture.

The most relevant parts of docker to use are the Dockerfile, docker images, and containers.

For our purposes, you should think of a docker container as a standalone, running virtual machine (or equivalently an AWS instance). Note this isn’t technically true and certainly shouldn’t be assumed for security but it’s effectively true from a development perspective. A docker container is an executable but can really be thought of as a file containing the full state of a machine. Because docker containers are executables, they exist in memory as a process (or group of processes).

We interact with a container primarily via `sudo docker {load, cp, run}`. `docker load` explicitly loads a docker image and creates a container (we’ll talk about what an image is exactly in a minute). `docker cp` copies a file/files from the local file system into the docker container (remember the container is a full standalone machine so it has its own filesystem). `docker run` runs a program in the vm. The program must already exist in the container and by default isn’t run interactively. For example, `docker run webserver nginx` will tell docker to load the image with the name `webserver` then run the `nginx` command (which will start a webserver). If you didn’t want to create a new container you could use `docker exec` instead. `docker run` is nice, however, because it makes reproducing things much much easier because each run is loaded fresh from an image. This is the advantage of docker (i.e. it is lightweight enough that loading a new container is inexpensive), you would never use this workflow with EC2 or VMWare. Note that containers are volatile. Changes made within a container are short lived and do not persist in a docker image are inherited in future containers

Docker images are the base image that a container is created from. This can be thought of as an AMI or snapshot in AWS, or a Live CD in the olden days. A docker image is effectively a read only file that effectively contains the state of a machine. This is what containers are loaded from. When a container is loaded, it can be thought of as making a copy of this image which it can then modify. Docker images are large but can be loaded quickly. They can be loaded multiple times. When a docker image is loaded into a container, the image is never modified and the changes to the container are not written back. This is what is distributed on dockerhub or in our projects will simply be distributed as a tarball.

A Dockerfile is recipe for building a docker image. It’s a small representation that can be used to build a docker image. This is highly analogous to the c/c++ build system with `make` (hence the similar name). A Dockerfile is converted to an image via `docker build`. Dockerfiles essentially specify how to build an image, usually using a previous image as a starting point. Like make files, a Dockerfile usually needs supplemental resources (i.e. source code for a program to install), these resources should be placed in the same directory as the Dockerfile and are called the context. They are not automatically loaded into the image. In order to successfully build an image, you need a Dockerfile and a context.

Basic syntax for a docker file includes `FROM`, `COPY`, `RUN`, and `CMD`. Dockerfiles should start with a `FROM` statements which specifies which base image to use. For example `FROM ubuntu` means that our docker image will be based on the ubuntu image (automatically searches dockerhub for image with this name). If we simply left our Dockerfile like this we would just build the existing ubuntu image. Customization can be done via the other commands. Essentially building an image is a matter of loading an existing image into a temporary container, modifying the container and saving it in a persistent manner (which is the new image). `COPY` will copy a file from your host operating system (the context) into the image’s file system. These files are persistent in the image. `RUN` will run a command within the container (i.e. an installation script). Finally `CMD` specifies a single command to be run when the image is finally loaded. It’s the first command executed by the container. If my docker file says `RUN foo`, `CMD bar`, then foo will be run exactly once (when I build the image) and bar will be run many times (once each time the container is loaded).

###The Launchpad Docker Workflow
Docker images should contain all dependencies needed for a project, but not necessarily the code.
You should be able to generate the docker image using just docker build and save.
Do most installation and setup via a script in the context (no reason to have docker on aws)
A single folder with a dockerfile and context should be uploadable to Google Drive for someone else to recreate the image.
The development environment should never include rebuilding an image (unless new dependencies are added).

###Optional stuff

Why is docker so frickin fast?

The easiest way to answer this question is to explain why tradition virtual machines are so slow. The traditional goal of a virtual machine has tended to be to provide an isolated and consistent environment. Isolation tend to be a word with security implications. An isolated process or OS should not be able to communicate with the host operating system or any other virtual machines. How do we do this in a guaranteed secure manner? Well the simplest way to do so (modern VMs simply do an optimized version of this) is to simply emulate hardware, or at least emulate the hardware/software boundary (no point in emulating caches). Naturally the most important thing to emulate quickly here is the CPU. Generally the approach here is to allow most native code execution. This tends to be ok because we already design operating systems to have process level isolation for non privileged processes (here privilege refers to kernel mode/ring 0 as opposed to running as root user). This primarily works because of address translation. This works for the vast majority of instructions and can be done by hardware. So what’s slow? There are a smaller class of behaviors that are much harder to deal with, primarily privilege escalating and privileged instructions). Because we blindly allow execution on the CPU, eventually our guest OS will likely try to escalate privilege (not necessarily malicious). This most commonly occurs as a syscall. We must then analyze the syscall, determine it came from the VM, go to the guest IVT, execute code, during which privileged instructions will likely be used (though this is now in user mode) which means more traps will be triggered, then the syscall stack will be popped. Notice how much longer this is than the normal syscall process. Not only is this more complicated, but it is also normally cached. Accessing the IVT is normally very fast (implemented in hardware). It likely won’t be accessed enough in a VM due to context switching so this will require going to memory. Each privileged instruction will also trap which would normally be executed seemlessly but now involves a trap. These traps and interrupts also provide a chance for context switching to happen which doubly sucks on a VM. When a context switch occurs, the host kernel scheduler may choose to schedule other processes first, then the guest kernel scheduler may choose to schedule other processes first. This means that from the perspective of a user in the guest vm, there are twice as many chances to get stuck waiting.

That was a long description of the problem, docker’s solution is simple. The vast majority of the slowdown was due to the guest kernel. It’s the only thing that executes privileged instructions and it’s where the extra context switching overhead comes into play. So let’s not have a guest kernel. The entire reason all this trapping business occurred was to ensure that the guest OS stays in its own memory. With modern linux we now have a feature called namespaces. Namespaces are a way of isolating memory of process groups. This is how docker works. A docker container is run in its own separate namespace(s). It is sometimes possible to do IPC across namespaces as well as access shared hardware resources quickly but also insecurely.
